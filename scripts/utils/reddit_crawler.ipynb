{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.12/site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "import sys\n",
    "import pandas as pd\n",
    "!pip3 install praw\n",
    "import praw\n",
    "import time\n",
    "sys.path.append('../') \n",
    "import config.settings as settings\n",
    "from pathlib import Path\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=settings.PRAW_CLIENT,\n",
    "    client_secret=settings.PRAW_CLIENT_SECRET,\n",
    "    user_agent=settings.PRAW_USER_AGENT\n",
    ")\n",
    "\n",
    "sleep_time = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Table Structure and Calling Reddit API\n",
    "Post limit sets to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_title = []\n",
    "post_id = []\n",
    "post_datetime = []\n",
    "post_score = []\n",
    "post_body = []\n",
    "comment_id = []\n",
    "comment_text = []\n",
    "comment_datetime = []\n",
    "comment_score = []\n",
    "seen_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fetched = 0\n",
    "target_posts = 900\n",
    "start_time = time.time()\n",
    "posts = reddit.subreddit(\"Taiwan\").new()\n",
    "\n",
    "for post in posts:\n",
    "        if post.id not in seen_ids and post.selftext:\n",
    "            print(f\"crawling comments from post {post.title}\")\n",
    "            seen_ids.add(post.id)\n",
    "            total_fetched += 1\n",
    "            post_title.append(post.title)\n",
    "            post_id.append(post.id)\n",
    "            post_datetime.append(post.created_utc)\n",
    "            post_score.append(post.score)\n",
    "            post_body.append(post.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_posts = 5\n",
    "start_time = time.time()\n",
    "posts = reddit.subreddit(\"Hongkong\").random_rising()\n",
    "total_fetched=0\n",
    "while total_fetched<=target_posts:\n",
    "    total_fetched += 1\n",
    "    for post in posts:\n",
    "        if post.id not in seen_ids and post.selftext:\n",
    "            print(f\"crawling from post {post.title}\")\n",
    "            seen_ids.add(post.id)\n",
    "            post_title.append(post.title)\n",
    "            post_id.append(post.id)\n",
    "            post_datetime.append(post.created_utc)\n",
    "            post_score.append(post.score)\n",
    "            post_body.append(post.selftext)\n",
    "        if total_fetched >= target_posts: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = reddit.subreddit(\"China\").rising()\n",
    "for post in posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "raw_data = {\n",
    "    \"post_title\": post_title,\n",
    "    \"post_id\": post_id,\n",
    "    \"post_body\": post_body,\n",
    "    \"post_datetime\": post_datetime,\n",
    "    \"post_score\": post_score,\n",
    "}\n",
    "\n",
    "analysis_path = Path.joinpath(settings.PROJ_PATH,\"analysis_data\")\n",
    "\n",
    "raw_data_df = pd.DataFrame(raw_data).drop_duplicates(subset=[\"post_id\"],keep=\"first\")\n",
    "raw_data_df.to_csv(Path.joinpath(analysis_path,\"/raw_data_hongkong_post_1.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling comments given postID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def fetch_comments_for_posts(reddit, df):\n",
    "    comment_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        post_id = row['post_id']\n",
    "        post_title = row['post_title']\n",
    "        post_body = row['post_body']\n",
    "        post_datetime = row['post_datetime']\n",
    "        post_score = row['post_score']\n",
    "\n",
    "        submission = reddit.submission(id=post_id)\n",
    "\n",
    "        # Load comments\n",
    "        submission.comments.replace_more(limit=0)  # To ensure all top-level comments are loaded\n",
    "        for comment in submission.comments:\n",
    "            comment_data.append({\n",
    "                'post_title': post_title,\n",
    "                'post_id': post_id,\n",
    "                'post_body': post_body,\n",
    "                'post_datetime': post_datetime,\n",
    "                'post_score': post_score,\n",
    "                'post_owner': submission.author.name if submission.author else None,\n",
    "                'comment_owner': comment.author.name if comment.author else None,\n",
    "                'reply_to_userId': comment.parent().author.name if comment.parent().author else None,\n",
    "                'comment_datetime': comment.created_utc,\n",
    "                'comment_score': comment.score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(comment_data)\n",
    "\n",
    "def process_files(data_path, output_path, reddit):\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(data_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            comments_df = fetch_comments_for_posts(reddit, df)\n",
    "            output_file = os.path.join(output_path, f\"{file.replace('raw_data_', '').replace('_post.csv', '')}_comments.csv\")\n",
    "            comments_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data_dir = Path.joinpath(settings.PROJ_PATH,\"post_data\")\n",
    "process_files(post_data_dir, post_data_dir, reddit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
